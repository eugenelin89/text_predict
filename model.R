library(dplyr)
source("scripts/gcd.R")


# input is ngram vector
learn_from_trigrams <- function(ngram){ 
  model <- list()
  for(i in 2: length(ngram)){
    rw <- ngram[i]
    str_vec <- strsplit(rw ,split=',', fixed=TRUE)[[1]] # e.g. [1] "99"   "at"   "the"  "time" "522"
    if(length(str_vec)!=5)
      next
    key <- paste(str_vec[2], str_vec[3], sep = " ")  #paste(rw$word1, rw$word2, sep = " ")
    val <- paste(rep(str_vec[4], strtoi(str_vec[5])), collapse = " ")  #paste(rep(rw$word3, rw$n), collapse = " ")
      
    cur_val <- model[[key]]
    if(is.null(cur_val)){
      # new entry
      model[key] = val
    }else{
      model[key] = paste(c(cur_val, val), collapse = " ")
    }

    if(i%%1000 == 0){
      print(i) # for visual inspection
    }

  }
  model
}

learn_from_bigrams <- function(ngram){ 
  model <- list()
  for(i in 2: length(ngram)){ #skip header
  #for(i in 2: 1000){
    rw <- ngram[i]
    str_vec <- strsplit(rw ,split=',', fixed=TRUE)[[1]] # e.g. [1] "99"   "at"   "the"  "time" "522"
    key <- str_vec[2]  #paste(rw$word1, rw$word2, sep = " ")
    
    n <- strtoi(str_vec[4])
    if(is.na(n))
      next
    
    val <- paste(rep(str_vec[3], strtoi(str_vec[4])), collapse = " ")  #paste(rep(rw$word3, rw$n), collapse = " ")
    
    cur_val <- model[[key]]
    if(is.null(cur_val)){
      # new entry
      model[key] = val
    }else{
      model[key] = paste(c(cur_val, val), collapse = " ")
    }
    
    if(i%%1000 == 0){
      print(i) # for visual inspection
    }
    
  }
  model
}

# for random word generation
learn_from_tokens <- function(tokens, model_size){
  model <- vector(mode = "character", length = model_size)
  head = 1
  for(i in 2 : length(tokens)){
    rw <- tokens[i]
    str_vec <- strsplit(rw ,split=',', fixed=TRUE)[[1]]
  
    if(length(str_vec) != 3){
      n <- strtoi(str_vec[length(str_vec)])
      word <- paste(str_vec[2:(length(str_vec)-1)], collapse = '')
    }else{
      n <- strtoi(str_vec[3])
      word <- str_vec[2]      
    }
    vec <- rep(word, n)
    model[head:(head+n-1)] <- vec
    head = head+n
    if(i%%1000 == 0)
      print(i)
  }

  model
}

# long_string generated by ngram model
predict_with_ngram<-function(long_string){
  result = NULL
  if(!is.null(long_string)){
    vec <- strsplit(long_string, split=" ", fixed=TRUE)[[1]]
    result <- vec[as.integer(runif(1, 1, length(vec)))]
  }
  result
}

predict_next_word <- function(sentence){
  # make sure the following models are loaded:
  # trigrams_model, bigrams_model, random_model
  sentence <- tolower(gsub('[[:punct:] ]+',' ',sentence))
  vec <- strsplit(sentence, split=" ", fixed=TRUE)[[1]]
  
  if(length(vec) == 0)
    return("")
  
  word1 = ""
  word2 = ""
  clean = character(0)
  for(i in length(vec) : 1){
    if(vec[i] != ""){
      clean <- append(clean, vec[i])
    }  
  }
  
  val <- NULL
  
  if(length(clean) >= 2){
    # trigram model
    key <- paste(clean[2:1], collapse = " ")
    val <- trigrams_model[[key]]
    result <- predict_with_ngram(val)
  }
  if(is.null(val) && length(clean) >= 1){
    # bigram model
    key <- clean[1]
    val <- bigrams_model[[key]]
    result <- predict_with_ngram(val)
  }
  if(is.null(val)){
    # random generation
    result <- random_model[as.integer(runif(1, 1, length(random_model)))]
  }
  
  result
}


generate_sentence <- function(input, n=15, result=null){
  if(n == 0)
    return(result)
  else{
    nxt <- predict_next_word(input)
    generate_sentence(paste(input, nxt), n-1, paste(input, nxt))
  }
}

#bigrams <- readRDS("bigrams.rds")
#trigrams <- readRDS("trigrams.rds")
#ngrams_model <- learn_from_ngrams(bigrams, trigrams)
#next_word <- predict_next_word(ngrams_model, "thanks for")

# generating trigram model
trigrams <- readRDS("trigrams.rds") # Read trigrams.rds generated by eda.R into dataframe
write.csv(trigrams, file="trigrams.csv", quote=FALSE) # Write trigrams dataframe to CSV
sample_text("trigrams.csv", "trigrams_sample.csv", prob=0.01) # Sampling of trigrams CSV (to .1)
trigrams_sample <- readLines("trigrams_sample.csv") # Read sample into vector
trigrams_model <- learn_from_trigrams(trigrams_sample) # from sample vector, learn the model, which is just a List.
save(trigrams_model, file="trigrams_model.RData") # save the trigram model to disk
load("trigrams_model.RData") # load trigram model

# generating bigram model
bigrams <- readRDS("bigrams.rds")
write.csv(bigrams, file="bigrams.csv", quote=FALSE)
sample_text("bigrams.csv", "bigrams_sample.csv", prob=0.01)
bigrams_sample <- readLines("bigrams_sample.csv")
bigrams_model <- learn_from_bigrams(bigrams_sample)
save(bigrams_model, file="bigrams_model.RData") 
load("bigrams_model.RData")

# random word generator
tokens <- readRDS("tokens.rds")
write.csv(tokens, file="tokens.csv", quote=FALSE)
token_sample <- readLines("tokens.csv")
random_model <- learn_from_tokens(token_sample, sum(tokens$n))
save(random_model, file="random_model.RData")
load("random_model.RData")

